# -*- coding: utf-8 -*-
"""Welcome To Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb
"""

import pandas as pd
import numpy as np
import re
import string
import glob
import os
import pickle
from collections import Counter

# NLP libraries
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# Feature extraction
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler

# Clustering
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.metrics import silhouette_score, davies_bouldin_score

# Dimensionality reduction
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Download required NLTK data (run once)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('punkt_tab') # Ensure punkt_tab is downloaded

# ============================================
# STEP 1: DATA LOADING
# ============================================

def load_news_data(csv_path='list.csv', text_folder='*.txt'):
    """Load news articles from CSV and text files"""
    # Initialize df outside try block
    df = pd.DataFrame()
    documents = []

    try:
        # Load the CSV with document IDs
        df = pd.read_csv(csv_path)
        print(f"Loaded {len(df)} entries from CSV")

        valid_documents_from_csv = []
        valid_ids_from_csv = []

        # Attempt to load text files based on CSV entries
        for idx, row in df.iterrows():
            doc_id = row['document_id']
            newsgroup = row['newsgroup']

            # Try to find the text file
            possible_paths = [
                f"{doc_id}.txt",
                f"documents/{doc_id}.txt",
                f"data/{doc_id}.txt",
                f"{newsgroup}/{doc_id}.txt"
            ]

            text = None
            for path in possible_paths:
                if os.path.exists(path):
                    try:
                        with open(path, 'r', encoding='latin-1', errors='ignore') as f: # Consistent encoding
                            text = f.read()
                        break
                    except Exception as e:
                        continue

            if text:
                valid_documents_from_csv.append(text)
                valid_ids_from_csv.append(idx)

        documents = valid_documents_from_csv
        # Filter the dataframe to only include rows for which we found text files
        if valid_ids_from_csv:
            df = df.iloc[valid_ids_from_csv].reset_index(drop=True)
        else:
            print("CSV found, but no matching text files based on CSV entries. Falling back to glob search.")
            df = pd.DataFrame() # Clear df if no text files found via CSV
            documents = [] # Ensure documents is empty to trigger glob fallback

    except FileNotFoundError:
        print(f"Warning: CSV file '{csv_path}' not found. Attempting to load all .txt files directly from current directory.")
        df = pd.DataFrame() # Clear df if CSV not found, will be populated from glob later

    # Fallback: If no documents were loaded yet (either CSV not found, or CSV found but no matches),
    # load all .txt files using glob.
    if not documents:
        print(f"Loading all .txt files from '/content/{text_folder}'...")
        filenames = []
        for filepath in glob.glob(os.path.join('/content', text_folder)): # Explicitly look in /content
            try:
                with open(filepath, 'r', encoding='latin-1', errors='ignore') as f: # Consistent encoding
                    documents.append(f.read())
                filenames.append(os.path.basename(filepath))
            except Exception as e:
                print(f"Error reading {filepath} during glob search: {e}")

        # If documents were loaded this way, create a basic DataFrame for them
        if documents:
            # Create dummy document_id and newsgroup if no CSV was provided
            doc_ids = []
            news_groups = []
            for i, fn in enumerate(filenames):
                parts = fn.replace('.txt', '').split('.')
                doc_ids.append(fn.replace('.txt', '')) # Use full filename as doc_id
                if len(parts) > 1:
                    news_groups.append(parts[0]) # Heuristic for newsgroup from filename
                else:
                    news_groups.append('unknown')
            df = pd.DataFrame({'document_id': doc_ids, 'newsgroup': news_groups, 'filename': filenames})

    if not documents:
        print("No documents loaded at all. Please check the 'list.csv' path or ensure '.txt' files are present in /content.")
        return pd.DataFrame(), []

    # Ensure the dataframe has a 'cluster' column to avoid issues later in the pipeline
    if 'cluster' not in df.columns:
        df['cluster'] = -1 # Default value

    print(f"Successfully loaded {len(documents)} documents. ")
    return df, documents

# ============================================
# STEP 2: TEXT PREPROCESSING
# ============================================

class TextPreprocessor:
    def __init__(self):
        self.lemmatizer = WordNetLemmatizer()
        self.stop_words = set(stopwords.words('english'))
        # Add common words that don't add meaning
        self.stop_words.update(['said', 'would', 'could', 'also', 'may', 'might'])

    def clean_text(self, text):
        """Basic cleaning"""
        # Convert to lowercase
        text = text.lower()

        # Remove URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '', text)

        # Remove email addresses
        text = re.sub(r'\S+@\S+', '', text)

        # Remove numbers
        text = re.sub(r'\d+', '', text)

        # Remove punctuation
        text = text.translate(str.maketrans('', '', string.punctuation))

        # Remove extra whitespace
        text = ' '.join(text.split())

        return text

    def tokenize_and_lemmatize(self, text):
        """Tokenize and lemmatize text"""
        # Tokenize
        tokens = word_tokenize(text)

        # Remove stopwords and lemmatize
        lemmatized = [
            self.lemmatizer.lemmatize(token) # Corrected from self.lemmatize
            for token in tokens
            if token not in self.stop_words and len(token) > 2
        ]

        return ' '.join(lemmatized)

    def preprocess(self, text):
        """Complete preprocessing pipeline"""
        text = self.clean_text(text)
        text = self.tokenize_and_lemmatize(text)
        return text

    def preprocess_documents(self, documents):
        """Preprocess multiple documents"""
        print("Preprocessing documents...")
        processed = [self.preprocess(doc) for doc in documents]
        print("Preprocessing complete!")
        return processed

# ============================================
# STEP 3: FEATURE EXTRACTION
# ============================================

def extract_tfidf_features(documents, max_features=1000, ngram_range=(1, 2)):
    """Convert text to TF-IDF vectors"""
    print(f"Extracting TF-IDF features (max_features={max_features})...")

    vectorizer = TfidfVectorizer(
        max_features=max_features,
        ngram_range=ngram_range,
        min_df=2, # Ignore terms that appear in less than 2 documents
        max_df=0.8 # Ignore terms that appear in more than 80% of documents
    )

    X = vectorizer.fit_transform(documents)
    feature_names = vectorizer.get_feature_names_out()

    print(f"Feature matrix shape: {X.shape}")
    return X.toarray(), feature_names, vectorizer

# ============================================
# STEP 4: CLUSTERING ALGORITHMS
# ============================================

def apply_kmeans(X, n_clusters=5):
    """Apply K-Means clustering"""
    print(f"\nApplying K-Means with {n_clusters} clusters...")
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X)

    # Calculate metrics
    silhouette = silhouette_score(X, labels)
    davies_bouldin = davies_bouldin_score(X, labels)

    print(f"Silhouette Score: {silhouette:.3f}")
    print(f"Davies-Bouldin Score: {davies_bouldin:.3f}")

    return labels, kmeans

def apply_hierarchical(X, n_clusters=5):
    """Apply Hierarchical clustering"""
    print(f"\nApplying Hierarchical clustering with {n_clusters} clusters...")
    hierarchical = AgglomerativeClustering(n_clusters=n_clusters)
    labels = hierarchical.fit_predict(X)

    silhouette = silhouette_score(X, labels)
    davies_bouldin = davies_bouldin_score(X, labels)

    print(f"Silhouette Score: {silhouette:.3f}")
    print(f"Davies-Bouldin Score: {davies_bouldin:.3f}")

    return labels, hierarchical

def apply_dbscan(X, eps=0.5, min_samples=5):
    """Apply DBSCAN clustering"""
    print(f"\nApplying DBSCAN (eps={eps}, min_samples={min_samples})...")
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    labels = dbscan.fit_predict(X)

    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    n_noise = list(labels).count(-1)

    print(f"Number of clusters: {n_clusters}")
    print(f"Number of noise points: {n_noise}")

    if n_clusters > 1:
        silhouette = silhouette_score(X[labels != -1], labels[labels != -1])
        print(f"Silhouette Score: {silhouette:.3f}")

    return labels, dbscan

# ============================================
# STEP 5: VISUALIZATION
# ============================================

def visualize_clusters_tsne(X, labels, title="t-SNE Cluster Visualization"):
    """Visualize clusters using t-SNE"""
    print("\nReducing dimensions with t-SNE...")

    # Ensure perplexity is less than n_samples
    n_samples = X.shape[0]
    # Default perplexity is 30, but it must be less than n_samples.
    # A common practice is to choose a value between 5 and 50.
    # We'll cap it at n_samples - 1 if it's too high.
    perplexity_val = min(30, n_samples - 1)

    if perplexity_val <= 0: # Handle cases with very few samples
        print("Warning: Not enough samples for t-SNE. Skipping visualization.")
        return None

    # Apply t-SNE
    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_val)
    X_tsne = tsne.fit_transform(X)

    # Create plot
    plt.figure(figsize=(12, 8))
    scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1],
                         c=labels, cmap='viridis',
                         alpha=0.6, s=50)
    plt.colorbar(scatter, label='Cluster')
    plt.title(title, fontsize=16)
    plt.xlabel('t-SNE Component 1')
    plt.ylabel('t-SNE Component 2')
    plt.tight_layout()
    plt.show()

    return X_tsne

def visualize_clusters_pca(X, labels, title="PCA Cluster Visualization"):
    """Visualize clusters using PCA"""
    print("\nReducing dimensions with PCA...")

    # Apply PCA
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)

    # Create plot
    plt.figure(figsize=(12, 8))
    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1],
                         c=labels, cmap='viridis',
                         alpha=0.6, s=50)
    plt.colorbar(scatter, label='Cluster')
    plt.title(title, fontsize=16)
    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')
    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')
    plt.tight_layout()
    plt.show()

    return X_pca

# ============================================
# STEP 6: TOPIC/KEYWORD EXTRACTION
# ============================================

def extract_top_keywords(X, feature_names, labels, n_keywords=10):
    """Extract top keywords for each cluster"""
    print("\n" + "="*60)
    print("TOP KEYWORDS PER CLUSTER")
    print("="*60)

    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    cluster_keywords = {}

    for cluster_id in range(n_clusters):
        # Get documents in this cluster
        cluster_mask = labels == cluster_id
        cluster_docs = X[cluster_mask]

        # Calculate mean TF-IDF for this cluster
        mean_tfidf = cluster_docs.mean(axis=0)

        # Get top keywords
        top_indices = mean_tfidf.argsort()[-n_keywords:][::-1]
        top_keywords = [feature_names[i] for i in top_indices]

        cluster_keywords[cluster_id] = top_keywords

        print(f"\nCluster {cluster_id} ({cluster_mask.sum()} documents):")
        print(f"Keywords: {', '.join(top_keywords)}")

    return cluster_keywords

def analyze_cluster_topics(documents, labels, n_samples=3):
    """Show sample documents from each cluster"""
    print("\n" + "="*60)
    print("SAMPLE DOCUMENTS PER CLUSTER")
    print("="*60)

    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)

    for cluster_id in range(n_clusters):
        cluster_mask = labels == cluster_id
        cluster_indices = np.where(cluster_mask)[0]

        print(f"\n--- Cluster {cluster_id} ---")
        for idx in cluster_indices[:n_samples]:
            preview = documents[idx][:150].replace('\n', ' ')
            print(f"  â€¢ {preview}...")

# ============================================
# STEP 7: MODEL SAVING AND LOADING
# ============================================

def save_model(model, vectorizer, preprocessor, cluster_keywords, filename='news_clustering_model.pkl'):
    """Save trained model and components"""
    model_data = {
        'model': model,
        'vectorizer': vectorizer,
        'preprocessor': preprocessor,
        'cluster_keywords': cluster_keywords
    }

    with open(filename, 'wb') as f:
        pickle.dump(model_data, f)

    print(f"\nModel saved to '{filename}'")

def load_model(filename='news_clustering_model.pkl'):
    """Load trained model and components"""
    with open(filename, 'rb') as f:
        model_data = pickle.load(f)

    print(f"Model loaded from '{filename}'")
    return model_data['model'], model_data['vectorizer'], model_data['preprocessor'], model_data['cluster_keywords']

# ============================================
# STEP 8: PREDICTION FOR NEW TEXT
# ============================================

class NewsClusterPredictor:
    """Class to predict cluster for new news articles"""

    def __init__(self, model, vectorizer, preprocessor, cluster_keywords):
        self.model = model
        self.vectorizer = vectorizer
        self.preprocessor = preprocessor
        self.cluster_keywords = cluster_keywords

    def predict_single(self, text):
        """Predict cluster for a single news article"""
        # Preprocess text
        processed_text = self.preprocessor.preprocess(text)

        # Convert to TF-IDF vector
        X_new = self.vectorizer.transform([processed_text]).toarray()

        # Predict cluster
        cluster = self.model.predict(X_new)[0]

        return cluster

    def predict_with_details(self, text):
        """Predict cluster and return detailed information"""
        cluster = self.predict_single(text)

        # Get cluster keywords
        keywords = self.cluster_keywords.get(cluster, [])

        # Calculate confidence (distance to cluster center for K-Means)
        processed_text = self.preprocessor.preprocess(text)
        X_new = self.vectorizer.transform([processed_text]).toarray()

        confidence = None
        if hasattr(self.model, 'cluster_centers_'):
            # For K-Means, calculate distance to cluster center
            distances = np.linalg.norm(self.model.cluster_centers_ - X_new, axis=1)
            confidence = 1 / (1 + distances[cluster]) # Convert distance to confidence score

        result = {
            'cluster': int(cluster),
            'keywords': keywords,
            'confidence': float(confidence) if confidence else None,
            'preview': text[:200] + '...' if len(text) > 200 else text
        }

        return result

    def predict_batch(self, texts):
        """Predict clusters for multiple texts"""
        results = []
        for text in texts:
            result = self.predict_with_details(text)
            results.append(result)
        return results

    def display_prediction(self, text):
        """Predict and display results in a user-friendly format"""
        print("\n" + "="*60)
        print("CLUSTER PREDICTION")
        print("="*60)

        result = self.predict_with_details(text)

        print(f"\nInput Text Preview:")
        print(f"  {result['preview']}")
        print(f"\nPredicted Cluster: {result['cluster']}")
        print(f"\nCluster Keywords:")
        print(f"  {', '.join(result['keywords'])}")

        if result['confidence']:
            print(f"\nConfidence Score: {result['confidence']:.3f}")

        print("="*60)

        return result

# ============================================
# MAIN PIPELINE
# ============================================

def run_clustering_pipeline(csv_path='list.csv', n_clusters=5, algorithm='kmeans', save_model_flag=True):
    """Complete clustering pipeline with model saving"""
    print("="*60)
    print("NEWS ARTICLE CLUSTERING PIPELINE")
    print("="*60)

    # Step 1: Load data
    df, documents = load_news_data(csv_path)

    # Step 2: Preprocess
    preprocessor = TextPreprocessor()
    processed_docs = preprocessor.preprocess_documents(documents)

    # Step 3: Extract features
    X, feature_names, vectorizer = extract_tfidf_features(processed_docs, max_features=500)

    # Step 4: Apply clustering
    if algorithm == 'kmeans':
        labels, model = apply_kmeans(X, n_clusters)
    elif algorithm == 'hierarchical':
        labels, model = apply_hierarchical(X, n_clusters)
    elif algorithm == 'dbscan':
        labels, model = apply_dbscan(X, eps=0.5, min_samples=5)
    else:
        raise ValueError("Algorithm must be 'kmeans', 'hierarchical', or 'dbscan'")

    # Step 5: Visualize
    visualize_clusters_tsne(X, labels, f"News Clusters ({algorithm.upper()})")

    # Step 6: Extract topics
    cluster_keywords = extract_top_keywords(X, feature_names, labels, n_keywords=10)
    analyze_cluster_topics(documents, labels, n_samples=2)

    # Add cluster labels to dataframe
    df['cluster'] = labels

    # Step 7: Save model
    if save_model_flag:
        save_model(model, vectorizer, preprocessor, cluster_keywords)

    print("\n" + "="*60)
    print("CLUSTERING COMPLETE!")
    print("="*60)

    return df, labels, X, feature_names, model, vectorizer, preprocessor, cluster_keywords

# ============================================
# EXAMPLE USAGE
# ============================================

if __name__ == "__main__":
    # ===== TRAINING PHASE =====
    print("PHASE 1: TRAINING THE MODEL")
    print("-" * 60)

    # Run the complete pipeline and save the model
    df_results, labels, X, features, model, vectorizer, preprocessor, cluster_keywords = run_clustering_pipeline(
        csv_path='list.csv',
        n_clusters=5,
        algorithm='kmeans',
        save_model_flag=True
    )

    # Save results
    df_results.to_csv('clustered_results.csv', index=False)
    print("\nResults saved to 'clustered_results.csv'")

    print("\n" + "="*60)
    print("PHASE 2: PREDICTION ON NEW TEXT")
    print("="*60)

    # Create predictor
    predictor = NewsClusterPredictor(model, vectorizer, preprocessor, cluster_keywords)

    # Example 1: Predict single article
    new_article = """
    The stock market showed strong gains today as technology companies reported
    better-than-expected earnings. The S&P 500 rose 2.5% while the Nasdaq composite
    gained 3.1%. Investors are optimistic about the economic recovery and corporate profits.
    """

    result1 = predictor.display_prediction(new_article)

    # Example 2: Another article
    print("\n")
    new_article2 = """
    Scientists have discovered a new species of deep-sea fish in the Mariana Trench.
    The fish, which lives at depths of over 8,000 meters, has unique adaptations that
    allow it to survive extreme pressure and cold temperatures. Researchers believe
    this discovery could provide insights into evolution and adaptation.
    """

    result2 = predictor.display_prediction(new_article2)

    # Example 3: Batch prediction
    print("\n" + "="*60)
    print("BATCH PREDICTION")
    print("="*60)

    batch_articles = [
        "The president announced new healthcare policies during today's press conference.",
        "Machine learning algorithms are revolutionizing data analysis in healthcare.",
        "The championship game went into overtime with an exciting finish."
    ]

    batch_results = predictor.predict_batch(batch_articles)
    for i, result in enumerate(batch_results):
        print(f"\nArticle {i+1} -> Cluster {result['cluster']}")
        print(f"Keywords: {', '.join(result['keywords'][:5])}")

    print("\n" + "="*60)
    print("PREDICTION COMPLETE!")
    print("="*60)

    # ===== USER INPUT MODE =====
    print("\n\nWould you like to predict a custom article? (y/n)")
    user_choice = input().strip().lower()

    if user_choice == 'y':
        print("\nEnter your news article text (press Enter twice when done):")
        lines = []
        while True:
            line = input()
            if line == "":
                break
            lines.append(line)

        user_text = " ".join(lines)

        if user_text:
            predictor.display_prediction(user_text)

